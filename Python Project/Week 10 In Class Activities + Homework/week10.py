# -*- coding: utf-8 -*-
"""Week10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AzLQv6xMDLmFmNh8E_YHG6RmEcw_1XWO

## Week 10 -- Regression & Binary Classification
Learning Objectives:


*   Plot and analyse data using MatplotLib
*   Get familiar with Numpy
*   Understand how linear regression and multiple regressions works
*   Understand how K-NN algorithm works
*   Use SKlearn to perform regression and binary classification using KNN

# Question 0
If we would like to predict whether a cancer cell is malignant (+ve) or benign (-ve), 
Which one is more detrimental:
1. False positive? 
2. False negative?
"""

import numpy as np
from sklearn.metrics import confusion_matrix

def get_metrics(actual_targets, predicted_targets, labels):
    
    c_matrix = confusion_matrix(actual_targets, predicted_targets, labels)
    
    total_rec = len(predicted_targets)
    correct_pred = 0
    last_row = 0
    top_row = 0
    
    for i in range(len(labels)):
        correct_pred += c_matrix[i][i]
        last_row += c_matrix[-1][i]
        top_row += c_matrix[0][i]
    
    output = {
        'confusion matrix': c_matrix,
        'total records': total_rec,
        'accuracy': round(correct_pred / total_rec, 3),
        'sensitivity': round(c_matrix[-1][-1] / last_row, 3),

        # dividing predicted benign / actual malignant >> which is what we want, because thats the more concerning one
        # so in this case, the word "positive" is benign ???
        'false positive rate': round(c_matrix[0][1] / top_row, 3) 
    }

    # malignant = 0
    # benign = 1 
    # top_row is the number of total actual malignant cells

    #                         # negative           # positive
    #                        predicted malignant    predicted benign
    # actual malignant           #c_matrix[0][0]     #c_matrix[0][1]   #negative
    # actual benign              #c_matrix[1][0]     #c_matrix[1][1]  #positive
    
    return output
  

actual = ['cat', 'cat', 'cat', 'cat', 'bird', 'bird','bird','bird'] 
predicted = ['cat', 'cat', 'bird', 'bird', 'cat', 'bird', 'bird', 'bird' ]
labels = ['bird', 'cat']
print(get_metrics(actual, predicted, labels) )

"""# Question 2
Five number summary
* The function is to take in a 2D numpy array
* If it is otherwise, it is to return None 
* Plotting the boxplot is made an optional part of the question. 
* There is no need to submit the code for the boxplot

Students should plot the boxplot first, students do not need to submit this code.
"""

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn import neighbors, datasets
from sklearn.metrics import confusion_matrix

bunchobject = datasets.load_breast_cancer()

#bunchobject.data is like a 2D array with 569 rows and 30 columns
first_column = bunchobject.data[:,[1]] 
plt.boxplot(first_column)
plt.show()


def five_number_summary(x):
    '''takes in a numpy array 
    returns a five number summary for each column 
    as a dictionary'''

    row, column = x.shape
    output = []

    for i in range(column): # this will iterate from 1st column to nth column
        five_number_dictionary = {
            'minimum': np.min(x[:,i]), 
            'first quartile': np.percentile(x[:,i], 25),
            'median': np.median(x[:,i]),
            'third quartile': np.percentile(x[:,i],75),
            'maximum': np.max(x[:,i])
        }
        output.append(five_number_dictionary)

    return output



print("TEST 1----")
first_column = bunchobject.data[:,[1]]
print( five_number_summary(first_column) ) 

print("TEST 2----")
col_no = [0,1,2]
some_columns = bunchobject.data[:,col_no] 
print( five_number_summary(some_columns) )

print("=====END OF QN 2======")

"""## Question 3
Min/Max normalization <br>
The students ought to be able to work this out from a linear equation. <br>
In any case, the formula is x* = (x - xmin)/(xmax - xmin)
"""

def normalize_minmax(data):
  size = data.shape 
  
  number_of_columns = size[1]

  for i in range(number_of_columns):
    # get max and min per column
    max_val = np.max(data[:,i]) 
    min_val = np.min(data[:,i])

    # normalise the entire column
    data[:,i] = (data[:,i] - min_val) / (max_val-min_val)

  
    
  return data



first_column = bunchobject.data[:,[1]]
print('normalized', five_number_summary(first_column))
xx = normalize_minmax(first_column)
print('normalized', five_number_summary(xx))


# cols = [1, 7]
# some_columns = bunchobject.data[:,cols]
# snorm = normalize_minmax(some_columns)
# print('normalized', five_number_summary(snorm))

"""# Question 4
k-Nearest Neighbours classification

students should plot a bar chart at the beginning of this question

No need to submit to vocareum
"""

def display_bar_chart(positions, counts, names, title_name='default' ):
    plt.bar(positions, counts, align='center')
    plt.xticks(positions, names)
    plt.title(title_name)
    plt.show()



# remember that bunchobject has 569 rows and 30 columns
# that means I have 569 SAMPLES, each sample having 30 FEATURES
# each sample also has a TARGET: either malignant or benign
# SO out of 569 SAMPLES, I have ~220 malignant cell, and ~350 benign cells, both making up to 569 SAMPLES


unique, counts = np.unique(bunchobject.target, return_counts = True)
display_bar_chart(unique, counts, bunchobject.target_names)



from sklearn.model_selection import train_test_split 

def knn_classifier(bunchobject, feature_list, size, seed , k ): 
    # 1. 569 samples >>> breaking them into 0.4 test set ~220 ish for test set and the rest is for "training set"
    # 2. for each of the sample in test set: we want to get the "closest" neighbour (3 closest neighbours since k = 3)
    # 3. Closest neighbour means: those with closest 20 features to the sample compared currently in 2
    # 4. The class of this test sample will be the class (majority) of the closest 3 training set obtained
    data = bunchobject.data[:, feature_list]
    target = bunchobject.target

    #always normalise your data before you perform classification training 
    data = normalize_minmax(data)

    data_train, data_test, target_train, target_test = train_test_split(data, target, test_size = size, random_state = seed)

    # initialize our classifier
    knn_classifier = neighbors.KNeighborsClassifier(k)
    knn_classifier.fit(data_train, target_train)
    predicted_target = knn_classifier.predict(data_test)

    results = get_metrics(target_test, predicted_target, [0,1])

    return results



features = range(20) #specifying that the first 20 features are included in the model
print(bunchobject.target)
print(bunchobject.target_names)
results = knn_classifier(bunchobject, features, 0.40, 2752, 20)

# if k = 3: {'confusion matrix': array([[ 73,   9],
#       [  5, 141]]), 'total records': 228, 'accuracy': 0.939, 'sensitivity': 0.966, 'false positive rate': 0.11}

# if k = 5: {'confusion matrix': array([[ 76,   6],
#       [  4, 142]]), 'total records': 228, 'accuracy': 0.956, 'sensitivity': 0.973, 'false positive rate': 0.073}

# if k = 10: {'confusion matrix': array([[ 75,   7],
#       [  5, 141]]), 'total records': 228, 'accuracy': 0.947, 'sensitivity': 0.966, 'false positive rate': 0.085}

# if k = 20: {'confusion matrix': array([[ 71,  11],
#       [  6, 140]]), 'total records': 228, 'accuracy': 0.925, 'sensitivity': 0.959, 'false positive rate': 0.134}
print(results)

"""# Question 5
Display scatter and perform linear regression using SKlearn
"""

def display_scatter(x,y, xlabel='x', ylabel='y',title_name ='default'): 
    plt.scatter(x,y)
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)
    plt.title(title_name)
    plt.show()


x_index = 0
y_index = 3

x = bunchobject.data[:,x_index] 
y = bunchobject.data[:,y_index]
print(bunchobject.feature_names)

x_label = bunchobject.feature_names[x_index]
y_label = bunchobject.feature_names[y_index]

display_scatter(x,y,x_label,y_label)

from sklearn import linear_model 
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split 

def linear_regression(bunchobject, x_index, y_index, size, seed):
    
    data = bunchobject.data 
    x = data[:, np.newaxis, x_index]
    y = data[:, np.newaxis, y_index]

    x_train, x_test, y_train, y_test = train_test_split( x , y , test_size = size, random_state = seed)
    
    regr = linear_model.LinearRegression() 
    regr.fit(x_train, y_train)
    y_pred = regr.predict(x_test)

    mse = mean_squared_error(y_test, y_pred)
    var = r2_score(y_test, y_pred)

    results = {
        'coefficients': regr.coef_,
        'intercept': regr.intercept_,
        'mean squared error': mse,
        'r2 score': var
    }

    return x_train, y_train, x_test, y_pred, results



def plot_linear_regression(x1, y1, x2, y2, x_label='', y_label=''):
    plt.scatter(x1,y1, color='black')
    plt.scatter(x2,y2, color='blue')
    plt.xlabel(x_label)
    plt.ylabel(y_label)
    plt.show() 



try:
    x_train, y_train, x_test, y_pred, results = linear_regression(bunchobject,0,3,0.4,2752)
    print(results)
    plot_linear_regression(x_train, y_train, x_test, y_pred, 
                            bunchobject.feature_names[0], 
                            bunchobject.feature_names[3])
  
except:
    print("The function has an error, it is supposed to return 5 things");

"""# Question 6
Multiple Linear Regression
"""

from sklearn.preprocessing import PolynomialFeatures

def multiple_linear_regression(bunchobject, x_index, y_index, order, size, seed):
    
    data = bunchobject.data 
    x = data[:, np.newaxis, x_index]
    y = data[:, np.newaxis, y_index]

    poly = PolynomialFeatures(order,include_bias=False)  # order = 3

    x_new_transformed = poly.fit_transform(x) # size is row equals to row of x, and col# == order

    x_new_transformed_train, x_new_transformed_test, y_train, y_test = train_test_split( x_new_transformed , y , test_size = size, random_state = seed)
    
    regr = linear_model.LinearRegression() 
    regr.fit(x_new_transformed_train, y_train)
    y_pred = regr.predict(x_new_transformed_test)

    mse = mean_squared_error(y_test, y_pred)
    var = r2_score(y_test, y_pred)

    results = {
        'coefficients': regr.coef_, # returns order low to high, coefficients from X to X^n
        'intercept': regr.intercept_,
        'mean squared error': mse,
        'r2 score': var
    }

    return x_new_transformed_train, y_train, x_new_transformed_test, y_pred, results



try:
    x_train, y_train, x_test, y_pred, results = multiple_linear_regression(bunchobject,0,0,3,0.4,2752)
    print(results)
    # plot_linear_regression(x_train, y_train, x_test, y_pred, 
    #                         bunchobject.feature_names[0], 
    #                         bunchobject.feature_names[3])
except:
    print("The function has an error, it is supposed to return 5 things");

"""# Question 7 
Complete the k-NN classifier function from Qn 4
"""

def knn_classifier_full(bunchobject, feature_list, size, seed): 
    data = bunchobject.data[:, feature_list]
    target = bunchobject.target

    #always normalise your data before you perform classification training 
    data = normalize_minmax(data)

    data_train, data_mid, target_train, target_mid = train_test_split(data, target, test_size = size, random_state = seed)
    data_validation, data_test, target_validation, target_test = train_test_split(data_mid, target_mid, test_size = 0.5, random_state = seed)

    # try running the validation set with various values of k 
    # store the k value that gives us the highest accuracy

    best_k_so_far = 1
    best_accuracy = -1
    best_predicted_target = 0

    for k in range(1, 20):
      # initialize our classifier
      knn_classifier = neighbors.KNeighborsClassifier(k) # initialize with different values of k
      knn_classifier.fit(data_train, target_train)
      
      predicted_target = knn_classifier.predict(data_validation)
      results = get_metrics(target_validation, predicted_target, [1,0])
      accuracy_k = results['accuracy']

      if (accuracy_k > best_accuracy):
        best_k_so_far = k
        best_accuracy = accuracy_k
        best_predicted_target = predicted_target 
    

    print("Best k so far: ")
    print(best_k_so_far) 

    # test with our test set, this is done only ONCE with test set
    knn_classifier = neighbors.KNeighborsClassifier(best_k_so_far) # initialize with different values of k
    knn_classifier.fit(data_train, target_train)
    predicted_target = knn_classifier.predict(data_test)

    results_with_test_set = get_metrics(target_test, predicted_target, [1,0])
    results_with_validation_set = get_metrics(target_validation, best_predicted_target, [1,0])

    results = {
        'best k': best_k_so_far,
        'validation set': results_with_validation_set,
        'test set': results_with_test_set
    }

    return results



# In[ ]:


results = knn_classifier_full(bunchobject, features, 0.40, 2752)
print(results)

